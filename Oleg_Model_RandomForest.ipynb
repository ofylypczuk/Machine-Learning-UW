{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# University of Warsaw - Machine Learning in Finance I\n",
                "## Final Project: Predictive Maintenance\n",
                "\n",
                "Objective: Build a model to predict machine failure (Binary Classification).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: Importing Libraries\n",
                "\n",
                "We are importing the standard stack, but most importantly SMOTE from imblearn. We need this specific library because our dataset is highly imbalanced, and standard methods won't work well without synthetic oversampling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "# importing everything at once so I don't forget later"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Loading Data\n",
                "\n",
                "Loading the dataset directly from the local project folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loading the csv from the data folder\n",
                "path = r\"c:\\Users\\Oleg\\.antigravity\\ML_project\\data\\ai4i2020.csv\"\n",
                "df = pd.read_csv(path)\n",
                "\n",
                "# checking if it actually loaded\n",
                "print(f\"Loaded dataset from {path}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Data Cleaning (Removing Leakage)\n",
                "\n",
                "We are dropping the ID columns because they are irrelevant. More importantly, we must drop the failure type columns (TWF, HDF, etc.).\n",
                "\n",
                "These columns represent Data Leakage. If we include HDF (Heat Dissipation Failure), the model isn't predicting; it's simply reading the diagnosis flag. Excluding these ensures the model learns from the sensor readings, not the component flags."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dropping the cheat codes (Leakage)\n",
                "# If I keep HDF, the model knows the machine failed instantly.\n",
                "df = df.drop(['UDI', 'Product ID', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1)\n",
                "\n",
                "# checking shape to make sure columns are gone\n",
                "print(\"New shape:\", df.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3.5: Feature Engineering (Bonus)\n",
                "\n",
                "Adding a new feature: Temperature Difference.\n",
                "\n",
                "Machines typically fail when they cannot dissipate heat. The difference between the Process Temperature and Air Temperature is a better indicator of this problem than the raw temperatures alone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Engineering (The \"A+\" Move)\n",
                "# My idea: Machines fail when they can't cool down.\n",
                "# So the DIFFERENCE between Process Temp and Air Temp is key.\n",
                "df['Temp_Diff'] = df['Process temperature [K]'] - df['Air temperature [K]']\n",
                "\n",
                "print(\"Added new feature: Temp_Diff\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Preprocessing (Encoding & Splitting)\n",
                "\n",
                "We use LabelEncoder to convert the Type column to numeric, as Random Forest requires numerical input.\n",
                "\n",
                "For splitting, we use a stratified split (stratify=y). This is crucial because our failures are rare (approx 3%). A random split might result in a test set with zero failures, which would make our evaluation meaningless."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Machines hate letters, so encoding Type to numbers\n",
                "le = LabelEncoder()\n",
                "df['Type'] = le.fit_transform(df['Type'])\n",
                "\n",
                "X = df.drop('Machine failure', axis=1)\n",
                "y = df['Machine failure']\n",
                "\n",
                "# Professor Wozniak said use stratify or we fail the project\n",
                "# Splitting 80/20\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
                "\n",
                "print(\"Training data shape:\", X_train.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 5: Handling Imbalance (SMOTE)\n",
                "\n",
                "We apply SMOTE (Synthetic Minority Over-sampling Technique) to the training data. The model usually biases heavily towards the majority class (No Failure). SMOTE forces the model to learn the minority class boundaries by generating synthetic examples, preventing it from just guessing zero every time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# fixing imbalance because reality is cruel and failures are rare\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
                "\n",
                "print(\"After SMote, we have this many failures:\", sum(y_train_resampled == 1))\n",
                "# now it should be balanced"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 6: Design Choices (Why we did it this way)\n",
                "\n",
                "1. Why SMOTE over Undersampling?\n",
                "- Undersampling: Deleting 90% of our data (the healthy machines) to match the failing ones. This throws away valuable information.\n",
                "- SMOTE: Keeps all original data and creates new failure data. This is better for learning complex patterns.\n",
                "\n",
                "2. Why Stratified Split over Random Split?\n",
                "- Random Split: Might accidentally put ALL failure cases in the training set, leaving none for testing.\n",
                "- Stratified: Guarantees that the Test set has exactly ~3.4% failures, just like the real world.\n",
                "\n",
                "3. Why Random Forest?\n",
                "- Random Forest uses \"Bagging\" (averaging many trees) which is robust against noise and overfitting, unlike a single deep Decision Tree."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model trained. Hope it learned something.\n"
                    ]
                }
            ],
            "source": [
                "# Building the Forest\n",
                "# limiting max_depth to 10 to prevent overfitting (memorizing the noise)\n",
                "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
                "\n",
                "rf.fit(X_train_resampled, y_train_resampled)\n",
                "print(\"Model trained. Hope it learned something.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 7: Evaluation (Standard 50% Threshold)\n",
                "\n",
                "We evaluate using ROC-AUC and Recall. In a predictive maintenance context, accuracy is misleading due to imbalance. We care most about Recall (catching true failures) because a False Negative (missed failure) is much more expensive than a False Positive (unnecessary inspection)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ROC-AUC Score:\n",
                        "0.952350505419559\n",
                        "\n",
                        "Classification Report (Standard 0.5 Threshold):\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.99      0.95      0.97      1932\n",
                        "           1       0.34      0.78      0.47        68\n",
                        "\n",
                        "    accuracy                           0.94      2000\n",
                        "   macro avg       0.67      0.86      0.72      2000\n",
                        "weighted avg       0.97      0.94      0.95      2000\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "y_pred = rf.predict(X_test)\n",
                "y_prob = rf.predict_proba(X_test)[:, 1]\n",
                "\n",
                "print(\"ROC-AUC Score:\")\n",
                "print(roc_auc_score(y_test, y_prob))\n",
                "\n",
                "print(\"\\nClassification Report (Standard 0.5 Threshold):\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 8: Advanced Tuning (Threshold & Visualization)\n",
                "\n",
                "Threshold Tuning (Business Logic):\n",
                "The standard threshold is 0.5. However, because breakdowns are so expensive, we want to be more paranoid. We lower the threshold to 0.3. This increases Recall (catching more failures) at the cost of some Precision (more false alarms).\n",
                "\n",
                "Feature Importance:\n",
                "We visualize which sensors contribute most to the model's decision."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Business Logic (Threshold Tuning)\n",
                "# Standard threshold is 0.5. But missing a failure is expensive.\n",
                "# So I am lowering the bar to 0.3. Better safe than sorry.\n",
                "custom_threshold = 0.3\n",
                "y_pred_tuned = (y_prob >= custom_threshold).astype(int)\n",
                "\n",
                "print(f\"\\n--- Results with Custom Threshold ({custom_threshold}) ---\")\n",
                "print(confusion_matrix(y_test, y_pred_tuned))\n",
                "print(classification_report(y_test, y_pred_tuned))\n",
                "\n",
                "\n",
                "# Visualizing what actually matters\n",
                "importances = rf.feature_importances_\n",
                "features = X.columns\n",
                "\n",
                "# plotting it nicely\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(x=importances, y=features)\n",
                "plt.title(\"What breaks the machine?\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
